{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cc12f64-a666-44ae-a29b-cd80c8b0afe3",
      "metadata": {
        "id": "7cc12f64-a666-44ae-a29b-cd80c8b0afe3"
      },
      "source": [
        "# Image classification using neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a4b4ae-7f61-418e-925d-f66772c831b7",
      "metadata": {
        "id": "53a4b4ae-7f61-418e-925d-f66772c831b7"
      },
      "source": [
        "Deep neural networks are the current best approach in artificial intelligence and machine learning systems. Machine learning is often called \"deep learning\" because the learning happens in deep neural networks, with many artificial neurons between the input and the output.\n",
        "\n",
        "A neural network is based on an analogue of the brain, with _neurons_ (nerve cells) connected to each other in a _network_. Each neuron takes a bunch of inputs (whether from the inputs to the network or from other neurons), adds them up, and uses that to generate an output. Those outputs then feed into either other neurons or the output of the network as a whole.\n",
        "\n",
        "Each neuron has several parameters. Each input to the neuron is weighted, and it's these weighted inputs that are added. Each neuron also has a bias, that acts as a threshold value for whether it generates an output.\n",
        "\n",
        "These neurons are typically organised in _layers_, with neurons in one layer feeding forward into the next. The diagram below shows a network of a few _dense_ layers, where each input to the layer feeds into each neuron.\n",
        "\n",
        "![A sample neural network](https://github.com/NeilNjae/ou-click-start-ai/blob/main/4.image-classification/pic/sample_neural_network.png?raw=1)\n",
        "\n",
        "To do image classification with this type of neural network, each pixel in the image is one input. The value of each pixel feeds into each neuron in the first layer, and so on through the network. The final layer has one neuron per class the network knows about, and the values at these outputs is the probability that this image is an example of each of the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47544aac-8d5f-469f-bc33-af2b66908c75",
      "metadata": {
        "id": "47544aac-8d5f-469f-bc33-af2b66908c75"
      },
      "source": [
        "When it's first created, the network has random weights throughout, so generates random classifications for each input. This is generally not what we want.\n",
        "\n",
        "To train the network, we need a large collection of images, each with its _label_ of the true class for that image. During training, we feed in an image and compare the output generated by the network to the output we want (the image's label). We can then see how the network's actual output differs from the desired output, and find the _error_ (difference) between them. The actual output is a combination of the weights and inputs of the final layer of neurons. We can combine the error and the weights to work out how to adjust the weights going into the output layer, and how we should change the values on the neurons that feed into that layer. That gives us an error in the next-to-last layer of neurons; we can use that to adjust the weights feeding into that layer, and so on back to the input layer.\n",
        "\n",
        "We then the next image from our training data and adjust the weights again.\n",
        "\n",
        "Once we've done all the images in the training set, we've completed one _epoch_ of training and the network is a bit better at classifying images.\n",
        "\n",
        "We then go through another epoch of training and the network hopefully improves a bit more. As we do more training, we get to the point of diminishing returns and we can stop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1381f0-d78a-4a65-856e-42221b9cfd18",
      "metadata": {
        "id": "7d1381f0-d78a-4a65-856e-42221b9cfd18"
      },
      "source": [
        "Enough talk. Let's build a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82813dc2-c3a0-4b45-813d-b54a15d251fe",
      "metadata": {
        "id": "82813dc2-c3a0-4b45-813d-b54a15d251fe"
      },
      "source": [
        "## Important: before you start\n",
        "Before you run any cells in this notebook, you need to ensure you're using a GPU. This will massively increase the speed of using the neural networks.\n",
        "\n",
        "Click on the Colab menu \"Runtime\". Select \"Change runtime type\". Select the \"T4\" GPU. This will restart the notebook, so you'll have lost any work you've already done.\n",
        "\n",
        "Once you're connected to a T4 runtime (you can see by looking at the top-right of the window, next to the \"RAM\" and \"Disk\" traces), you can start work in this noteobok."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f360ba-2ed9-4245-b3ef-257c81d5906a",
      "metadata": {
        "id": "69f360ba-2ed9-4245-b3ef-257c81d5906a"
      },
      "source": [
        "# Loading some data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6011ff-5d0c-4377-ae65-7015a5dc8ed2",
      "metadata": {
        "id": "2c6011ff-5d0c-4377-ae65-7015a5dc8ed2"
      },
      "source": [
        "We need to train the neural network on some data, so we'll load some. We'll use the [standard CIFAR-10 set of small images](https://www.cs.toronto.edu/~kriz/cifar.html), in ten classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b00a96-adf8-4b3f-8094-6234db200f57",
      "metadata": {
        "id": "b9b00a96-adf8-4b3f-8094-6234db200f57"
      },
      "source": [
        "First, we import the libraries we'll use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ca3d6d",
      "metadata": {
        "id": "e0ca3d6d"
      },
      "outputs": [],
      "source": [
        "# Load some toolkits we will need later\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers, metrics, Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a629e0dc",
      "metadata": {
        "id": "a629e0dc"
      },
      "source": [
        "We set some parameters then load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12555eb0-63a5-42cc-89a8-ed29cd8f102a",
      "metadata": {
        "id": "12555eb0-63a5-42cc-89a8-ed29cd8f102a"
      },
      "outputs": [],
      "source": [
        "ROWS = 32\n",
        "COLS = 32\n",
        "CHANNELS = 3\n",
        "INPUT_SHAPE = (ROWS, COLS, CHANNELS)\n",
        "\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c54aa96",
      "metadata": {
        "id": "7c54aa96"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "(train_data, test_data), dataset_info = tfds.load('cifar10',\n",
        "    split=['train', 'test'],\n",
        "    with_info=True)\n",
        "train_data, test_data, dataset_info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666ba15e-98c5-482d-884e-9d64083f8e6e",
      "metadata": {
        "id": "666ba15e-98c5-482d-884e-9d64083f8e6e"
      },
      "source": [
        "We'll reformat the dataset into the form we need for the network to use. We split the data into a _training_ set, that we use to train the network; a _validation_ set, that we use to keep an eye on training as it progresses, and a _test_ set for our final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562e62d7",
      "metadata": {
        "id": "562e62d7"
      },
      "outputs": [],
      "source": [
        "# Preparing the data\n",
        "\n",
        "# define the list of class labels\n",
        "class_names =  ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "class_num = len(class_names)\n",
        "\n",
        "def ds_elem_transform(elem):\n",
        "    return (tf.cast(elem['image'], tf.float32) / 255, # convert pixel values to range 0-1\n",
        "           tf.one_hot(elem['label'], 10) # one-hot encoding for labels, 10 choices\n",
        "           )\n",
        "\n",
        "# Transform every element of the dataset, rename the result.\n",
        "train_validation_data = train_data.map(\n",
        "    ds_elem_transform, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Preparing the training and validation data\n",
        "# Take the elements at index 9, 19, 29... into the validation dataset\n",
        "validation_data = train_validation_data.shard(10, 9)\n",
        "validation_data = validation_data.batch(64)\n",
        "validation_data = validation_data.cache()\n",
        "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create a new train_data dataset. Append the remaining shards of the train_validation_data dataset.\n",
        "train_data = train_validation_data.shard(10, 8)\n",
        "for i in range(8):\n",
        "    train_data = train_data.concatenate(train_validation_data.shard(10, i))\n",
        "\n",
        "train_data = train_data.cache()\n",
        "train_data = train_data.shuffle(dataset_info.splits['train'].num_examples)\n",
        "train_data = train_data.batch(64)\n",
        "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Perform the same steps on the test dataset:\n",
        "# transform each element, batch, cache, and prefetch.\n",
        "test_data = test_data.map(\n",
        "    ds_elem_transform, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_data = test_data.batch(64)\n",
        "test_data = test_data.cache()\n",
        "test_data = test_data.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d4d837-bc41-48eb-b78b-d33e9301a677",
      "metadata": {
        "id": "13d4d837-bc41-48eb-b78b-d33e9301a677"
      },
      "source": [
        "How much data do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072deb45",
      "metadata": {
        "id": "072deb45"
      },
      "outputs": [],
      "source": [
        "len(train_data), len(validation_data), len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5fe1d26-d0a0-4990-80d8-44ab94967cb0",
      "metadata": {
        "id": "c5fe1d26-d0a0-4990-80d8-44ab94967cb0"
      },
      "source": [
        "That's 704 _batches_ of training data, with 64 images per batch, meaning there are just over 45,000 images to train from."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9149db3c-f6ef-4fc0-91d6-f99ebb236f32",
      "metadata": {
        "id": "9149db3c-f6ef-4fc0-91d6-f99ebb236f32"
      },
      "source": [
        "### Viewing some pictures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7851a632-f2bb-4668-8b8f-034f9a7c23ca",
      "metadata": {
        "id": "7851a632-f2bb-4668-8b8f-034f9a7c23ca"
      },
      "source": [
        "We use the `matploblib` library to show 25 sample images in the `train_data` we have loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de7a204",
      "metadata": {
        "id": "7de7a204"
      },
      "outputs": [],
      "source": [
        "# Get the first batch.\n",
        "# (This may take a moment as it reads and caches the Dataset)\n",
        "sample_imgs, sample_labels = train_data.as_numpy_iterator().next()\n",
        "\n",
        "# The canvas size is first initialized by setting the parameter figsize in function plt.figure()\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)  # draw five rows and five columns in canvas\n",
        "    plt.imshow(sample_imgs[i], cmap=plt.cm.binary)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.title(class_names[np.argmax(sample_labels[i])])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683f6cfc-eff6-43c4-8cbe-c590878a381b",
      "metadata": {
        "id": "683f6cfc-eff6-43c4-8cbe-c590878a381b"
      },
      "source": [
        "# Implementing a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a874bc-f975-465c-b0a3-6e419b1ee4db",
      "metadata": {
        "id": "93a874bc-f975-465c-b0a3-6e419b1ee4db"
      },
      "source": [
        "We now implement a straightforward neural network. We have an input layer, a `Flatten` layer that converts the 32×32×3 image into a set of 3027 values, and two `Dense` layers to do the classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47fd96d-0851-4441-8f49-0c4defe359ad",
      "metadata": {
        "id": "d47fd96d-0851-4441-8f49-0c4defe359ad"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(INPUT_SHAPE),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(class_num, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aaf8026-2aee-499b-b6f1-d78888e3a9a2",
      "metadata": {
        "id": "3aaf8026-2aee-499b-b6f1-d78888e3a9a2"
      },
      "source": [
        "The summary shows the structure of the network and how many parameters it has. In this case, just over three-quarters of a million."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c465880c-2dbd-4d8b-a86f-da43091f3251",
      "metadata": {
        "id": "c465880c-2dbd-4d8b-a86f-da43091f3251"
      },
      "source": [
        "We `compile` the model with some parameters to control how training progresses, then we `fit` the model to the training data. The validation data isn't used to train the network. Instead, after each epoch of training, we evalaute the model on the validation data to see how well it performs on other data. This helps when it comes to overfitting, as we'll see later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56bf3ea-01db-4f8c-9e6f-b9dd7235cc1c",
      "metadata": {
        "id": "b56bf3ea-01db-4f8c-9e6f-b9dd7235cc1c"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    'SGD',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "529424ca-fde7-4b65-b622-4e64176d1061",
      "metadata": {
        "id": "529424ca-fde7-4b65-b622-4e64176d1061"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=10,\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bccbd6e-5d3e-4366-8af5-8169e23b26b3",
      "metadata": {
        "id": "9bccbd6e-5d3e-4366-8af5-8169e23b26b3"
      },
      "source": [
        "We can see how the accuracy of the model changes over training. We can also see that the accuracy when using the validation data is similar to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8baf00cb-0e4e-4a62-93b1-dba07e4e81a8",
      "metadata": {
        "id": "8baf00cb-0e4e-4a62-93b1-dba07e4e81a8"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()  # Automatic detection of elements to be shown in the legend\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65fe013-9ce2-49cf-87ea-571049ec446a",
      "metadata": {
        "id": "f65fe013-9ce2-49cf-87ea-571049ec446a"
      },
      "source": [
        "### Making predictions on test data\n",
        "\n",
        "We can now use the model to make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ef8ee1-6eaf-4565-a55f-a5b5918c4d2f",
      "metadata": {
        "id": "47ef8ee1-6eaf-4565-a55f-a5b5918c4d2f"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_data)\n",
        "test_predictions.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd63d4f1-b9f4-4af1-8423-353fa7e5a596",
      "metadata": {
        "id": "dd63d4f1-b9f4-4af1-8423-353fa7e5a596"
      },
      "source": [
        "This gives us 10,000 predictions across ten classes. If we look at one of the predictions, we can see the probabilities of the different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1819381d-f11e-4fe3-a528-1fe52f18e6ad",
      "metadata": {
        "id": "1819381d-f11e-4fe3-a528-1fe52f18e6ad"
      },
      "outputs": [],
      "source": [
        "test_predictions[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b15fd71-9c59-4263-a525-f2e9ca0a9430",
      "metadata": {
        "id": "9b15fd71-9c59-4263-a525-f2e9ca0a9430"
      },
      "source": [
        "The predicted label is the index of the highest-valued output for each prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f751ab2c-6905-4012-aa56-32f05e56a882",
      "metadata": {
        "id": "f751ab2c-6905-4012-aa56-32f05e56a882"
      },
      "outputs": [],
      "source": [
        "predict_labels = np.argmax(test_predictions, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f50d2c-8257-4514-896a-af04d0f6e7e7",
      "metadata": {
        "id": "75f50d2c-8257-4514-896a-af04d0f6e7e7"
      },
      "source": [
        "Now we view some images and give the predicted and actual labels for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d4ffe3-6a7e-4363-a67d-5487c31630db",
      "metadata": {
        "id": "e2d4ffe3-6a7e-4363-a67d-5487c31630db"
      },
      "outputs": [],
      "source": [
        "# View the true and predicted labels of sample images\n",
        "plt.figure(figsize=(15,10))\n",
        "test_imgs, test_labels = test_data.as_numpy_iterator().next()\n",
        "\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_imgs[i], cmap=plt.cm.binary)\n",
        "    p_class = predict_labels[i]\n",
        "    a_class = np.argmax(test_labels[i])\n",
        "    plt.title(f\"P: {class_names[p_class]} (A: {class_names[a_class]})\",\n",
        "                                  color=(\"green\" if p_class == a_class else \"red\"))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52e75bc8-48ff-4571-b093-60fa5651bdbc",
      "metadata": {
        "id": "52e75bc8-48ff-4571-b093-60fa5651bdbc"
      },
      "source": [
        "Finally, we can evaluate the overall accuracy of the model on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba439f1-8e6e-425f-881a-fef3d47861a8",
      "metadata": {
        "id": "dba439f1-8e6e-425f-881a-fef3d47861a8"
      },
      "outputs": [],
      "source": [
        "model_results = model.evaluate(test_data, return_dict=True)\n",
        "model_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f575b5-ef30-439d-8041-7b05de608cde",
      "metadata": {
        "id": "d4f575b5-ef30-439d-8041-7b05de608cde"
      },
      "source": [
        "Your results will vary, as your neural network was given its own random weights at the start. But you should see an accuracy of around 35–40%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a8cd835-49da-4b18-94e3-85f3fb6f0bb0",
      "metadata": {
        "id": "1a8cd835-49da-4b18-94e3-85f3fb6f0bb0"
      },
      "source": [
        "# Why use CNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5082be2-3f9a-4174-ad04-7c9651b77a36",
      "metadata": {
        "id": "d5082be2-3f9a-4174-ad04-7c9651b77a36"
      },
      "source": [
        "The neural network above works, but not very well. Just under 40% accuracy, from a range on only ten classes, isn't great.\n",
        "\n",
        "One reason for this is that network throws away all the spatial information in that first `Flatten` layer. Subsequent layers no longer know about which pixels are nearby to other pixels in the original image. A better approach would be to have a neural network that looks at just small regions of the image and feeds that to the next layers.\n",
        "\n",
        "That is a convolutional neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9bd66bb-01c9-45f9-bfce-e967ca8f7583",
      "metadata": {
        "id": "c9bd66bb-01c9-45f9-bfce-e967ca8f7583"
      },
      "source": [
        "## Convolution layer\n",
        "\n",
        "The idea of the convolution layer is to create a filter (also called kernel). The filter is used to scan across the image and create a representation of the image corresponding to the filter. In this way, we can think of the filter as a specific feature extraction mechanism for the image. For instance, a filter may detect a block of colour, or a horizontal edge, or a curve.\n",
        "\n",
        "A single convoution layer may contain many filters (32 filters in the example below). That means the layer can detect 32 features in the image. The network learns which features are important during training.\n",
        "\n",
        "![Example filters](https://github.com/NeilNjae/ou-click-start-ai/blob/main/4.image-classification/pic/Example-filters.png?raw=1)\n",
        "\n",
        "If we use 32 different filters in the convolution layer, then we create 32 different representations, called **feature maps**, of the input image. These different feature maps in combination can help us identify the input image correctly.\n",
        "\n",
        "How a filter operates is illustrated below. Simply speaking, we overlay a filter, which is a small matrix of weights, on top of the input matrix, e.g. starting from the top-left corner and then sliding from left to right and from top to bottom.\n",
        "\n",
        "![An example convolution](https://github.com/NeilNjae/ou-click-start-ai/blob/main/4.image-classification/pic/example-convolution.png?raw=1)\n",
        "\n",
        "The **filter** shown in the example is of size *3×3*. It is applied on a *5×5* input matrix (you can consider it as the matrix of pixels for an image). When the filter moves through the image it does its computation in each position, then moves to cover another *3×3* part of the input. The right-hand side matrix shows the output of each step.\n",
        "\n",
        "*The values in the kernel are learnt during the training step.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac427ed",
      "metadata": {
        "id": "7ac427ed"
      },
      "source": [
        "# Building a CNN model\n",
        "Let's use a convolution layer for image recognition.\n",
        "\n",
        "We'll use a convolution layer that has 32 filters. We then `Flatten` those neurons into a set of 33,000 neurons, and use a couple of `Dense` layers to use those features and create the image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366d5fdc",
      "metadata": {
        "id": "366d5fdc"
      },
      "source": [
        "![A simple CNN](https://github.com/NeilNjae/ou-click-start-ai/blob/main/4.image-classification/pic/cnn-diagram-simple.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1b8bfe",
      "metadata": {
        "id": "df1b8bfe"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential([\n",
        "    Input(INPUT_SHAPE),\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "    # MaxPooling2D(pool_size=(2, 2)),\n",
        "    # Dropout(0.5),\n",
        "    Flatten(),\n",
        "    Dense(128),\n",
        "    Dense(class_num, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6167fd28-f87c-4dc4-989f-ea5dd207ede2",
      "metadata": {
        "id": "6167fd28-f87c-4dc4-989f-ea5dd207ede2"
      },
      "source": [
        "Again, we compile and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac28fac",
      "metadata": {
        "id": "5ac28fac"
      },
      "outputs": [],
      "source": [
        "model2.compile(\n",
        "    'adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7e0e88",
      "metadata": {
        "id": "0e7e0e88"
      },
      "outputs": [],
      "source": [
        "history = model2.fit(train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=10,\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e54c96a",
      "metadata": {
        "id": "5e54c96a"
      },
      "source": [
        "There's good news and bad news.\n",
        "\n",
        "The good news is that the accuracy is much better than when not using the convolution layer. The training accuracy will be about 85%, the validation accuracy about 55%.\n",
        "\n",
        "The bad news is that difference between the performance on the training and validation data. It's clearer if we plot the training history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767d321f",
      "metadata": {
        "id": "767d321f"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()  # Automatic detection of elements to be shown in the legend\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d24eff",
      "metadata": {
        "id": "26d24eff"
      },
      "source": [
        "This shows the model is _overfitting_: it's getting very good at classifying the training data, but does much less well on data it's not been trained on. (The validation data is used to check how well the model works, not to improve the model's performance.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55831403-cb33-4b5d-81f8-f6bdf1fe3cb8",
      "metadata": {
        "id": "55831403-cb33-4b5d-81f8-f6bdf1fe3cb8"
      },
      "source": [
        "## Addressing overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd5ac3b-5c83-4d2a-986b-bbf392ae6ac9",
      "metadata": {
        "id": "9bd5ac3b-5c83-4d2a-986b-bbf392ae6ac9"
      },
      "source": [
        "We can address overfitting by throwing away some of the information _inside_ the model during training. We do this in two ways: pooling and dropout.\n",
        "\n",
        "Pooling uses the observation that pieces of the image near each other are very similar, and will have very similar feature maps from a convolutional layer. In any given small region (\"pool\") of a feature map, we can take just one pixel to go forward. Generally, we take the largest value in the pool.\n",
        "\n",
        "Dropout just plain discards information during training. Networks can overfit if all the weights can be adjusted together. During training, a dropout layer will randomly turn off some of the connections with each training example. This allows the weights to change independently and therefore makes the network more robust."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d371052-3617-4bbf-8484-0c27a7f92e5d",
      "metadata": {
        "id": "1d371052-3617-4bbf-8484-0c27a7f92e5d"
      },
      "source": [
        "By combining both pooling and dropout layers, we can afford to make the network deeper before overfitting occurs. We've done this in the model below, where we have three convolution layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9696730-6a07-41c9-905d-1327dd29185e",
      "metadata": {
        "id": "e9696730-6a07-41c9-905d-1327dd29185e"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential([\n",
        "    Input((32, 32, 3)),\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(class_num, activation='softmax')\n",
        "])\n",
        "\n",
        "model3.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d58c56b-a6c7-46e7-8c21-8a62a416cd64",
      "metadata": {
        "id": "5d58c56b-a6c7-46e7-8c21-8a62a416cd64"
      },
      "outputs": [],
      "source": [
        "model3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af3a45c-2318-4874-949d-516749b43f67",
      "metadata": {
        "id": "5af3a45c-2318-4874-949d-516749b43f67"
      },
      "source": [
        "Note that the last feature map is only 4×4 across, but has 64 features to choose from."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da953d7-104e-47f1-bfb4-85572f90244b",
      "metadata": {
        "id": "4da953d7-104e-47f1-bfb4-85572f90244b"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132ed328-a25f-4760-8e3b-88a35420c176",
      "metadata": {
        "id": "132ed328-a25f-4760-8e3b-88a35420c176"
      },
      "source": [
        "We can now compile and train the model, this time for **30** epochs. Apart from the number of epochs, use exactly the same code as you did above for `model2`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed65f487-c2be-4a15-82ce-6cf7d118206d",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ed65f487-c2be-4a15-82ce-6cf7d118206d"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f188332-cda4-4439-87eb-f8b908d4e079",
      "metadata": {
        "id": "3f188332-cda4-4439-87eb-f8b908d4e079"
      },
      "outputs": [],
      "source": [
        "history = model3.fit(train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=30\n",
        "         )\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.figure();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a2fdbd7-4225-4658-b74e-ae063a64a695",
      "metadata": {
        "id": "3a2fdbd7-4225-4658-b74e-ae063a64a695"
      },
      "source": [
        "### End of solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da5199d1-34d7-407b-857f-f9b8eb2c2f30",
      "metadata": {
        "id": "da5199d1-34d7-407b-857f-f9b8eb2c2f30"
      },
      "source": [
        "This seems to have gone much better. The accuracy is better, and the accuracy of the validation dataset is very close to the training dataset.\n",
        "\n",
        "How does it work on the test dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e210bdd2-0247-4aef-8034-97b230b99c47",
      "metadata": {
        "id": "e210bdd2-0247-4aef-8034-97b230b99c47"
      },
      "outputs": [],
      "source": [
        "model3_results = model3.evaluate(test_data, return_dict=True)\n",
        "model3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff8d14c-8240-4613-a8a9-0864500c5dc3",
      "metadata": {
        "id": "5ff8d14c-8240-4613-a8a9-0864500c5dc3"
      },
      "source": [
        "Your results will vary, but you'll probably see an accuracy in the 70–75% range.\n",
        "\n",
        "We can show some images with their predicted and actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdb0531-9eae-4a54-b807-f676d18609e1",
      "metadata": {
        "id": "4fdb0531-9eae-4a54-b807-f676d18609e1"
      },
      "outputs": [],
      "source": [
        "test_predictions = model3.predict(test_data)\n",
        "\n",
        "predict_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# View the true and predicted labels of some sample images\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "test_imgs, test_labels = test_data.as_numpy_iterator().next()\n",
        "\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_imgs[i])\n",
        "    p_class = predict_labels[i]\n",
        "    a_class = np.argmax(test_labels[i])\n",
        "\n",
        "    plt.title(f\"P: {class_names[p_class]} (A: {class_names[a_class]})\",\n",
        "                                  color=(\"green\" if p_class == a_class else \"red\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5485334-4665-475b-80aa-7a8c0cccfb16",
      "metadata": {
        "id": "a5485334-4665-475b-80aa-7a8c0cccfb16"
      },
      "source": [
        "This shows the output of the model, but we can also peer inside it and look at the feature maps it generates.\n",
        "\n",
        "We do that by creating another \"model\" that reuses the same trained layers in `model3`. The input to this new model is the same as before, but the model's output is the output of the first convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b242bdac-946f-4ac0-907e-388540dda812",
      "metadata": {
        "id": "b242bdac-946f-4ac0-907e-388540dda812"
      },
      "source": [
        "First we need to check the locations of the convolutional layers in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65149892-7eff-4cf3-8a5d-9deb56fccd5f",
      "metadata": {
        "id": "65149892-7eff-4cf3-8a5d-9deb56fccd5f"
      },
      "outputs": [],
      "source": [
        "list(enumerate(model3.layers))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e2627f-6349-42c0-ae70-0ef26f336660",
      "metadata": {
        "id": "b0e2627f-6349-42c0-ae70-0ef26f336660"
      },
      "source": [
        "We now create the model with the same input and first convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50f7d97-de3a-4e67-95de-4c4b10cf3ac3",
      "metadata": {
        "id": "e50f7d97-de3a-4e67-95de-4c4b10cf3ac3"
      },
      "outputs": [],
      "source": [
        "intermediate_layer_model = keras.Model(inputs=model3.inputs,\n",
        "                                       outputs=model3.layers[0].output)\n",
        "intermediate_layer_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10f99a76-b4d9-423b-a66e-50c53e2a84a9",
      "metadata": {
        "id": "10f99a76-b4d9-423b-a66e-50c53e2a84a9"
      },
      "source": [
        "We now look at the feature maps for a particular image. We'll use image 0 from the test batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf57adf-9e0d-4bcc-9ea9-522091121ca8",
      "metadata": {
        "id": "faf57adf-9e0d-4bcc-9ea9-522091121ca8"
      },
      "outputs": [],
      "source": [
        "test_image_number = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9589103-582f-4303-9a9f-fa1391cc1b26",
      "metadata": {
        "id": "d9589103-582f-4303-9a9f-fa1391cc1b26"
      },
      "outputs": [],
      "source": [
        "# ploting the original image\n",
        "plt.figure()\n",
        "plt.imshow(test_imgs[test_image_number])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3337ea-618a-4a16-9895-d2be71344e2c",
      "metadata": {
        "id": "bd3337ea-618a-4a16-9895-d2be71344e2c"
      },
      "source": [
        "Now we can show some sample outputs of the intermediate model: the feature maps of this image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76587478-cc91-47fb-86d3-aa99df035823",
      "metadata": {
        "id": "76587478-cc91-47fb-86d3-aa99df035823"
      },
      "outputs": [],
      "source": [
        "intermediate_output = intermediate_layer_model(test_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892f771a-1b56-4e7b-87fc-bee46396e38f",
      "metadata": {
        "id": "892f771a-1b56-4e7b-87fc-bee46396e38f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i in range(32):\n",
        "    plt.subplot(4, 8, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(intermediate_output[test_image_number,:,:,i])\n",
        "    plt.xlabel(f'Filter {i}', fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8627f98c-bea1-48c7-825a-2966b79adc8f",
      "metadata": {
        "id": "8627f98c-bea1-48c7-825a-2966b79adc8f"
      },
      "source": [
        "I've no idea what features your model will find. But you'll probably see some feature maps that indicate foreground or background, and some that detect edges between them in different directions.\n",
        "\n",
        "These are the features that later stages in the model will work with."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc4750e-ae1d-4194-9ee3-bf0e36c44827",
      "metadata": {
        "id": "3cc4750e-ae1d-4194-9ee3-bf0e36c44827"
      },
      "source": [
        "Following the same process, we can look at the output of the second covolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3374074-752a-492c-aed0-5c283bec26a1",
      "metadata": {
        "id": "a3374074-752a-492c-aed0-5c283bec26a1"
      },
      "outputs": [],
      "source": [
        "intermediate_layer_model2 = keras.Model(inputs=model3.inputs,\n",
        "                                       outputs=model3.layers[2].output)\n",
        "intermediate_layer_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a12e21-fb59-4081-b849-d799bb8d2ae9",
      "metadata": {
        "id": "37a12e21-fb59-4081-b849-d799bb8d2ae9"
      },
      "outputs": [],
      "source": [
        "intermediate_output2 = intermediate_layer_model2(test_imgs)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i in range(32):\n",
        "    plt.subplot(4, 8, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(intermediate_output2[test_image_number,:,:,i])\n",
        "    plt.xlabel(f'Filter {i}', fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db0fe3f-7b2a-41dc-84d5-4dcf980501a4",
      "metadata": {
        "id": "5db0fe3f-7b2a-41dc-84d5-4dcf980501a4"
      },
      "source": [
        "These features will be more abstract than the ones before, as they're combinations of some or many of the features from the first convolutional layer.\n",
        "\n",
        "Finally, the third convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374fb1e4-ea12-4203-ac82-c44cb28791a6",
      "metadata": {
        "id": "374fb1e4-ea12-4203-ac82-c44cb28791a6"
      },
      "outputs": [],
      "source": [
        "intermediate_layer_model3 = keras.Model(inputs=model3.inputs,\n",
        "                                       outputs=model3.layers[4].output)\n",
        "intermediate_layer_model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723949a2-64e5-4148-b1a3-2e9aeb2bda00",
      "metadata": {
        "id": "723949a2-64e5-4148-b1a3-2e9aeb2bda00"
      },
      "outputs": [],
      "source": [
        "intermediate_output3 = intermediate_layer_model3(test_imgs)\n",
        "\n",
        "# Plotting the 8 intermediate feature maps generated by the 8 filters in the first convolution layer\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i in range(64):\n",
        "    plt.subplot(8, 8, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(intermediate_output3[test_image_number,:,:,i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7068059a-c6d8-4cfa-bd2f-fddfd0a218a1",
      "metadata": {
        "id": "7068059a-c6d8-4cfa-bd2f-fddfd0a218a1"
      },
      "source": [
        "These will bear very little direct connection to the input image, but should somehow encode important features that help the classification of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85747057-0a9b-4880-be0a-2694155192e7",
      "metadata": {
        "id": "85747057-0a9b-4880-be0a-2694155192e7"
      },
      "outputs": [],
      "source": [
        "test_image_number = 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851c1d32-e8f6-4052-b358-c2d4a8750b60",
      "metadata": {
        "id": "851c1d32-e8f6-4052-b358-c2d4a8750b60"
      },
      "outputs": [],
      "source": [
        "plt.imshow(test_imgs[test_image_number])\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(32):\n",
        "    plt.subplot(4, 8, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(intermediate_output[test_image_number,:,:,i])\n",
        "    # plt.xlabel(f'Filter {i}', fontsize=10)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(32):\n",
        "    plt.subplot(4, 8, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(intermediate_output2[test_image_number,:,:,i])\n",
        "    # plt.xlabel(f'Filter {i}', fontsize=10)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(64):\n",
        "    plt.subplot(8, 8, i+1)\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(intermediate_output3[test_image_number,:,:,i])\n",
        "    # plt.xlabel(f'Filter {i}', fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978c61b6",
      "metadata": {
        "id": "978c61b6"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52924e58",
      "metadata": {
        "id": "52924e58"
      },
      "source": [
        "With these activities, you've explored some of the techniques of modern machine learning and artificial intelligence. Deep convolutional neural networks are a mainstay of deployed AI systems and are used for all sorts of image processing tasks. Variations on these ideas can be applied to other types of data, such as understanding speech and text.\n",
        "\n",
        "You've also looked at how convolutional networks work internally, with the creation of feature maps that encode important parts of the image, and how these feature maps are combined to give higher-level representations of an input.\n",
        "\n",
        "You've also seen why these systems take a lot of time, computing power, and data to get good performance. The CIFAR dataset is only small, but required many rounds of training, on specific hardware, to get good results.\n",
        "\n",
        "CNNs are just one technique in machine learning, but the ideas you used here should give you a much better understanding of how these systems work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0045d3d6",
      "metadata": {
        "id": "0045d3d6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}